---
layout: post
title: 【经典算法问题】TOP K进阶之海量数据
gh-repo: youcoding98/youcoding98.github.io
gh-badge: [star, fork, follow]
tags: [Algorithm]
---
在大规模数据处理中，经常会遇到的一类问题：在海量数据中找出出现频率最好的前k个数，或者从海量数据中找出最大的前k个数，这类问题通常被称为top K问题。


## 一、海量数据处理
> 海量数据处理，无非就是基于海量数据上的存储、处理、操作。何谓海量，就是数据量太大，所以导致要么是无法在较短时间内迅速解决，要么是数据太大，导致无法一次性装入内存。  

**解决方法：**  
针对时间，我们可以采用巧妙的算法搭配合适的数据结构，如`Bloom filter/Hash/BitMap/堆/数据库或倒排索引/trie树`，  
针对空间，无非就一个办法：大而化小，分而治之（hash映射）
至于所谓的单机及集群问题，单机就是处理装载数据的机器有限(只要考虑cpu，内存，硬盘的数据交互)，  
而集群，机器有多辆，适合分布式处理，并行计算(更多考虑节点和节点间的数据交互)。

这里我们总结了六种方法来解决海量数据`TopK`问题：
1. 分而治之/`Hash`映射 + `Hash`统计 + 堆/快速/归并排序
2. 双层桶划分
3. `Bloom filter`/`Bitmap`
4. Trie树/数据库/倒排索引
5. 外排序
6. 分布式处理之`Hadoop`/`Mapreduce`

## 二、实现 
这里我们根据具体问题来阐述这六种解决方法  

### 1. 分而治之/`Hash`映射 + `Hash`统计 + 堆/快速/归并排序

**适用范围：快速查找，删除的基本数据结构，通常需要总数据量可以放入内存**

> 0、100w个数中找出最大的100个数。

方案1：**采用局部淘汰法**：选取前100个元素，并排序，记为序列L。然后一次扫描剩余的元素x，与排好序的100个元素中最小的元素比，如果比这个最小的要大，
那么把这个最小的元素删除，并把x利用插入排序的思想，插入到序列L中。依次循环，知道扫描了所有的元素。复杂度为`O(100w*100)`。  
方案2：**采用快速排序的思想**:每次分割之后只考虑比轴大的一部分，知道比轴大的一部分在比100多的时候，采用传统排序算法排序，取前100个。复杂度为`O(100w*100)`。  
方案3：**利用最小堆**，用一个含100个元素的最小堆完成。复杂度为`O(100w*lg100)`。

> 1、海量日志数据，提取出某日访问百度次数最多的那个IP

方案1：  
   - 将访问百度的日志中的IP取出来，逐个写入到一个大文件中。注意到IP是32位的，最多有个2^32个IP。  
   - 分而治之/Hash映射：采用映射的方法，比如%1000，把整个大文件映射为1000个小文件
   - HashMap统计：对那1000个文件中的所有IP进行频率统计，然后依次找出各个文件中频率最大的那个IP
   - 排序：统计完了之后，在这1000个最大的IP中，找出那个频率最大的IP。

> 2、寻找热门查询，300万个查询字符串中统计最热门的10个查询

虽然有一千万个Query，但是由于重复度比较高，因此事实上只有300万的Query，每个Query255Byte，因此我们可以考虑把他们都放进内存中去  
对于这种数据规模比较小，能一次性装入内存的问题，可以放弃分而治之/hash映射的步骤，直接上hash统计，然后排序  
方案1：  
   - 先对这批海量数据预处理，在O（N）的时间内用Hash表完成统计。  
   - 借助堆这个数据结构，找出Top K，时间复杂度为`NlogK`
方案2：  
   - 采用trie树，关键字域存该查询串出现的次数，没有出现为0。
   - 最后用10个元素的最小推来对出现频率进行排序。  

> 3、有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词。

思路同上面几题一致：分而治之/`Hash`映射 + `Hash`统计 + 堆/快速/归并排序
方案1：  
   - Hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件。如果有文件大小超过1M,则按照类似方法继续往下分。  
   - Hash统计：统计每个文件中出现的词以及相应的频率(可以采用`trie树`/`HashMap`等)，并取出出现频率最大的100个词(可以利用堆排序)，并把100个词及相应的频率存入文件，这样又得到了5000个文件；
   - 归并排序：把这5000个文件进行归并（类似与归并排序）的过程

> 4、海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。

方案1：  
   - Hash映射：重新hash取摸，如此使得同一个元素只出现在单独的一台电脑中  
   - Hash统计：统计每台电脑中出现的元素以及相应的频率(可以采用`trie树`/`HashMap`等)，并取出TOP 10(可以利用堆排序)
   - 排序：组合100台电脑上的TOP10，找出最终的TOP10。
> 5、有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序。

方案同上：query不能一次加入内存，则先进行Hash映射，按照hash(query)%10，将相同元素出现在同一文件中；如果可以一次加入内存，则直接利用HashMap或Trie树
进行统计频率，得到Top 10输出到文件中得到排好序的10个文件，最后对这10个文件进行归并排序
> 6、 给定a、b两个文件，各存放50亿个url，每个url各占64字节，内存限制是4G，让你找出a、b文件共同的url？

方案1：  
   - Hash映射： 遍历文件a，对每个url求取`hash(url)%1000`，然后根据所取得的值将url分别存储到1000个小文件，遍历文件b，采取和a相同的方式将url分别存储到1000小文件
   这样处理后，所有可能相同的url都在对应的小文件,然后我们只要求出1000对小文件中相同的url即可。  
   - HashSet统计：先把a小文件中的url存入HashSet，然后对 对应b小文件中的url进行遍历，判断是否存在
   
> 7、怎么在海量数据中找出重复次数最多的一个？  

方案一：
   - 先做hash，然后求模映射为小文件，求出每个小文件中重复次数最多的一个，并记录重复次数。然后找出上一步求出的数据中重复次数最多的一个就是所求
### 2. 多层划分

**适用范围：第k大，中位数，不重复或重复的数字**
基本原理及要点：因为元素范围很大，不能利用直接寻址表，所以通过多次划分，逐步确定范围，然后最后在一个可以接受的范围内进行。  
> 1、2.5亿个整数中找出不重复的整数的个数，内存空间不足以容纳这2.5亿个整数。

方案1：  
   - 有点像鸽巢原理，整数个数为2^32,也就是，我们可以将这2^32个数，划分为2^8个区域(比如用单个文件代表一个区域)，然后将数据分离到不同的区域，然后不同的区域在利用bitmap就可以直接解决了。

> 5亿个int找它们的中位数  

方案1：  
   - 首先我们将int划分为2^16个区域，
   - 然后读取数据统计落到各个区域里的数的个数
   - 之后我们根据统计结果就可以判断中位数落到那个区域，同时知道这个区域中的第几大数刚好是中位数。
   - 然后第二次扫描我们只统计落在这个区域中的那些数就可以了。  
### 3. `Bloom filter`/`Bitmap`

**适用范围：可以用来实现数据字典，进行数据的判重，或者集合求交集**

**1)Bloom filter**  
> 给你A,B两个文件，各存放50亿条URL，每条URL占用64字节，内存限制是4G，让你找出A,B文件共同的URL。如果是三个乃至n个文件呢？

方案1：  见上文问题6，利用分治(Hash映射)
方案2：
   - 采用布隆过滤器：将查询数据的id放进布隆过滤器里，当用户请求时，使用布隆过滤器进行判断该id是否存在布隆过滤器中，不存在直接返回null  
   - 会存在误报，假设布隆过滤器的错误率为0.01，则位数组大小m约为输入元素个数n的13倍，此时需要的哈希函数k约为8个。
   元素个数为5G，我们需要`5G * 13 = 65G = 650亿` 即需要650亿个bit位才能达到错误率0.01,而我们拥有的内存可容纳bit位个数为 320亿，按此实现错误率大于0.01

**2)BitMap**  

使用位图法判断**整形数组**是否存在重复     
BitMap就是用一个bit位来标记某个元素对应的Value， 而Key即是该元素。因此在存储空间方面，可以大大节省。 
> 在2.5亿个整数中找出不重复的整数，注，内存不足以容纳这2.5亿个整数。

方案一：采用2-Bitmap（每个数分配2bit，00表示不存在，01表示出现一次，10表示多次，11无意义）进行，共需内存2^32 * 2 bit=1 GB内存，还可以接受。
然后扫描这2.5亿个整数，查看Bitmap中相对应位，如果是00变01，01变10，10保持不变。所描完事后，查看bitmap，把对应位是01的整数输出即可。  
方案二：利用分治，进行划分小文件的方法。然后在小文件中找出不重复的整数，并排序。然后再进行归并，注意去除重复的元素。  
> 给40亿个不重复的unsigned int的整数，没排过序的，然后再给一个数，如何快速判断这个数是否在那40亿个数当中？

方案一：利用BitMap：申请512M的内存，一个bit位代表一个unsigned int值。读入40亿个数，设置相应的bit位，读入要查询的数，查看相应bit位是否为1，为1表示存在，为0表示不存在。    
方案二：因为2^32为40亿多，所以给定一个数可能在，也可能不在其中；这里我们把40亿个数中的每一个用32位的二进制来表示    
       然后将这40亿个数分成两类:1)最高位为0,2)最高位为1;并将这两类分别写入到两个文件中，其中一个文件中数的个数<=20亿，而另一个>=20亿（这相当于折半了）；  
       与要查找的数的最高位比较并接着进入相应的文件再查找;再然后把这个文件为又分成两类:1)次最高位为0,2)次最高位为1,并将这两类分别写入到两个文件中，其中一个文件中数的个数<=10亿，而另一个>=10亿  
       以此类推，就可以找到了,而且时间复杂度为O(logn)  

### 4. Trie树/数据库/倒排索引
**1)Trie树**  
适用范围：数据量大，重复多，但是数据种类小可以放入内存    
问题实例：  

> 上面的第2题：寻找热门查询：查询串的重复度比较高，虽然总数是1千万，但如果除去重复后，不超过3百万个，每个不超过255字节。  

> 上面的第5题：有10个文件，每个文件1G，每个文件的每一行都存放的是用户的query，每个文件的query都可能重复。要你按照query的频度排序。  

> 1000万字符串，其中有些是相同的(重复),需要把重复的全部去掉，保留没有重复的字符串。  

> 上面的第8题：一个文本文件，大约有一万行，每行一个词，要求统计出其中最频繁出现的前10个词。
    
方案一： 用trie树统计每个词出现的次数，时间复杂度是O(n*le)（le表示单词的平准长度），然后是找出出现最频繁的前10个词。  
**2)数据库**  
适用范围：大数据量的增删改查  
基本原理及要点：利用数据的设计实现方法，对海量数据的增删改查进行处理。  
**3)倒排索引**  
适用范围：搜索引擎，关键字查询  
基本原理及要点：为何叫倒排索引？一种索引方法，被用来存储在全文搜索下某个单词在一个文档或者一组文档中的存储位置的映射。  

正向索引开发出来用来存储每个文档的单词的列表，即文档指向了它包含的那些单词；反向索引则是单词指向了包含它的文档 
问题实例：  
> 文档检索系统，查询那些文件包含了某单词，比如常见的学术论文的关键字搜索。


### 5. 外排序

**适用范围：大数据的排序，去重**  
基本原理及要点：外排序的归并方法，置换选择败者树原理，最优归并树  
问题实例：  
> 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16个字节，内存限制大小是1M。返回频数最高的100个词。  

方案一：这个数据具有很明显的特点，词的大小为16个字节，但是内存只有1m做hash有些不够，所以可以用来排序。内存可以当输入缓冲区使用。  


### 5. 分布式处理之`Mapreduce`  
MapReduce是一种计算模型，简单的说就是将大批量的工作（数据）分解（MAP）执行，然后再将结果合并成最终结果（REDUCE）。
这样做的好处是可以在任务被分解后，可以通过大量机器进行并行计算，减少整个操作的时间。  

**适用范围：数据量大，但是数据种类小可以放入内存**  
基本原理及要点：将数据交给不同的机器去处理，数据划分，结果归约。  

问题实例：  
> 海量数据分布在100台电脑中，想个办法高效统计出这批数据的TOP10。 
 
>  一共有N个机器，每个机器上有N个数。每个机器最多存O(N)个数并对它们操作。如何找到N^2个数的中数  

## 参考文献
1. [教你如何迅速秒杀掉：99%的海量数据处理面试题](https://blog.csdn.net/v_JULY_v/article/details/7382693?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-2.control)
2. [十道海量数据处理面试题与十个方法大总结](https://blog.csdn.net/v_JULY_v/article/details/6279498)



